{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: K-Nearest-Neighbors\n",
    "\n",
    "Two plants that look very much alike probably represent the same species; likewise, it is quite common that patients complaining of similar symptoms suffer from the same disease. In short, similar objects often belong to the same class — an observation that forms the basis of a popular approach to classification: when asked to determine the class of object $x$ find the training example most similar to it. Then label $x$ with this example’s class.  This is the fundamental notion of ___K-Nearest-Neighbors___.\n",
    "\n",
    "# K-Nearest-Neighbors Algorithm\n",
    "\n",
    "Let's discuss the simplest version of the K-NN classifier algorithm.  Suppose we have a mechanism to evaluate the similarly between attribute vectors - such as Eucliedean distance. Let $x$ denote the object whose class we want to determine.\n",
    "\n",
    "1. Among the training examples, identify the $k$ nearest neighbors of $x$ (examples most similar to $x$). \n",
    "2. Let $c_i$ be the class most frequently found among these $k$ nearest neighbors.  \n",
    "3. Label $x$ with $c_i$.\n",
    "\n",
    "<img src=\"images/03_23.png\" alt=\"knn symbolic\" title=\"title\" width=400/>\n",
    "\n",
    "# The Need for Multiple Neighbors\n",
    "\n",
    "In noisy domains, the testimony of the nearest neighbor cannot be trusted. What if this single specimen’s class label is incorrect? A more robust approach identifies not one, but several nearest neighbors, and then lets them vote. This is the essence of the so-called k-NN classifier, where k is the number of the voting neighbors, which is a user-specified parameter.\n",
    "\n",
    "<img src=\"Figures/nearestNeighbors.png\" alt=\"knn symbolic\" title=\"title\" width=600/>\n",
    "\n",
    "# Measuring Similarity\n",
    "\n",
    "The most common similarity mesure is the Euclidean distance, defined as:\n",
    "\n",
    "$$ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2} $$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "# Irrelevant Attributes and Scaling Problems\n",
    "\n",
    "Some features are irrelevant in the sense that their values have nothing to do with the class to which the instance they describe belongs.\n",
    "\n",
    "Let’s consider an example. In the following figure, the training set examples are characterized by two numeric attributes: body-temperature (the horizontal axis) and shoe-size (the vertical axis). The black dot stands for the object that the K-NN classifier is expected to label as healthy (pos) or sick (neg) - that is, the black dot is the new instance that we want to make a prediction on.\n",
    "\n",
    "<img src=\"Figures/shoeSizeBodyTemp.png\" title=\"title\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Iris Data Set\n",
    "\n",
    "## Load Data\n",
    "\n",
    "Let's import the Iris dataset and try to predict the type of Iris represented by some particular set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# only use sklearn to grab the data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "    \n",
    "iris_df = pd.Dataframe(iris_data.data, columns = iris_data.frame_names)\n",
    "iris_df['iris_type'] = iris_data.target_names[iris_data.target]\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Similarity or Distance for a Test Point and all Training Data\n",
    "\n",
    "`np.linalg.norm(x)` for an one-dimensional array `x` is $\\sqrt{\\sum_{i=1}^n{x_i^2}}$ which is the Euclidean norm of the vector $x$\n",
    "\n",
    "`x_train` is a numpy $n \\times p$ matrix, where there are $n$ data points with $p$ features.  `test_row` is a numpy array of $p$ elements.  `x_train - test_row` automatically subtracts `test_row` from each row of `x_train`.  `np.linalg.norm(x, axis=1)` finds the Euclidean norm of each row and returns an numpy array of size $n$.\n",
    "\n",
    "`np.argsort` does not change the numpy array `distances`, but returns the set of indices so that `distances[sorted_row_indices]` would appear sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(x_train, test_row, num_neighbors):\n",
    "    distances = np.linalg.norm(x_train - test_row, axis = 1)\n",
    "    sorted_row_indices = np. argsort(distances)\n",
    "    return sorted_row_indices[:num_neighbors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction using 'Majority' Rule\n",
    "\n",
    "In the below, we use a [Python collection](https://docs.python.org/3/library/collections.html#module-collections) called [Counter()](https://docs.python.org/3/library/collections.html#collections.Counter) that takes a list (or actually, any type of iterable data structure) and creates a dictionary-like object having keys and values.  The keys (of the dictionary) correspond to the unique elements of the list passed to it, and the values (of the dictionary) are computed as the number of times the element appears.\n",
    "\n",
    "`y_train[neighbor_indices]` is the array of target values corresponding to the nearest neighbors.\n",
    "\n",
    "The method `most_common(k)` selects the k elements that have the highest counts, in descending order of counts.  `most_common(k)` returns a list of tuples of the form (element, count).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(x_train, y_train, test_row, num_neighbors):\n",
    "    neighbor_indices = get_neighbors(x_train, test_row, num_neighbors)\n",
    "    output_values_conter = Counter(y_train[neighbor_indices])\n",
    "    prediction = output_values_counter.most_common()\n",
    "    return prediction[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "\n",
    "# define the test_row\n",
    "row = [5.7, 2.9, 4.2, 1.3]\n",
    "\n",
    "# predict the label\n",
    "label = predict_classification(iris_data.data, iris_data.target, row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, iris_data.target_names[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Norm and Data Scaling\n",
    "\n",
    "Euclidean norm is a great choice when:\n",
    "\n",
    "1. All the features are 'real' numbers.\n",
    "2. All the features are in the same scale.\n",
    "\n",
    "In our case, that is true.  All features are 'real' and in centimeters and seem to be in the same scale.  _However, you should scale your data before using it!_\n",
    "\n",
    "Two options are to use a MinMax or StandardScaler classes or user-defined procedures.\n",
    "\n",
    "If `x` is a numpy array of size $n \\times p$, then `min_by_column` is numpy array of size $p$ which are the minimums for each column.  Likewise for `max_by_column`.\n",
    "\n",
    "As before, `x2 - min_by_column` subtracts the $p$ array `min_by_column` from each row of `x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "# dataset assumed to be a numpy array\n",
    "def normalize_array(x: np.array):\n",
    "    x2 = x.copy()\n",
    "    min_by_column = x2.min(axis = 0)\n",
    "    max_by_column = x2.max(axis = 0)\n",
    "    x2 = (x2 - min_by_column) / (max_by colmn - min_by_column)\n",
    "    return x2, min_by_column, max_by_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the label\n",
    "normalized_training_data, min_scaling_factor, max_scaling_factor = normalize_array(iris_data.data)\n",
    "normalized_row = (row -  min_scaling_factor) / ( max_scaling_factor -  min_scaling_factor)\n",
    "\n",
    "label = predict_classification(normalized_training_data.data, iris_data.target, normalized_row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, iris_data.target_names[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn: KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris_data.data)\n",
    "\n",
    "normalized_training_data = scaler.transform(iris_data.data)\n",
    "normalized_row = scaler.transform(np.array(row).reshape(1, -1))\n",
    "\n",
    "knn = KNeighborClassifier(n_neighbors = 5,\n",
    "                         p = 2, \n",
    "                         metric = 'minkowski')\n",
    "knn.fit(normalized_training_data, iris_data.target)\n",
    "\n",
    "label = knn.predict(normalized_row)\n",
    "print('Data=%s, Predicted: %s' % (row, iris_data.target_names[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Decision Region\n",
    "\n",
    "Let's display the decision regions predicted by the model.  We will only consider two dimensions, so let's work with petal length and petal width (the 3rd and 4th features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from Python Machine Learning, 2ed, Sebastian Raschka\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborClassifier(n_neighbors = 5,\n",
    "                         p = 2, \n",
    "                         metric = 'minkowski')\n",
    "X = normalized_training_data[:,[2,3]]\n",
    "knn2.fit(X, iris_data.target)\n",
    "\n",
    "plot_decision_regions(X, iris_data.target, \n",
    "                      classifier=knn2)\n",
    "\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: KNN for Spelling Correction\n",
    "\n",
    "Here, we ask the KNN algorithm to correct misspellings such as Holpful (should be Helpful).  To do so, we use a British-English [list of words](https://www.python-course.eu/british-english.txt) that was modified from [another source](https://www.python-course.eu/k_nearest_neighbor_classifier.php). \n",
    "\n",
    "Here, the input are not real numbers, but strings.  _So we cannot use Scikit-Learn, even with a custom distance function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.read_csv(r'data/british-english.txt', names=[\"word\"])\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 'levenshtein' distance metric.  Check out [this video](https://www.youtube.com/watch?v=MiqoA-yF-0M) to learn more about the levenshtein distance, which measures the cost of changing one string to another given three basic operations: delete, replace, insert.  It shows how many changes are needed to get from string `s` to string `t`.  The following implementation comes from [here](https://www.python-course.eu/levenshtein_distance.php), where you'll find more clever implementations.\n",
    "\n",
    "Note that in the following code uses [type annotations](https://dev.to/dstarner/using-pythons-type-annotations-4cfe), which can be summarized by the following two additions to a function definition:\n",
    "\n",
    "- After the name of an input argument is defined in the function signature, it is followed by a colon and its data type - the general usage being: `<varName> : <varType>`.  For example, `myString : str`.  This is done to make the code clearer to read, and reveal it's purpose a little more.\n",
    "\n",
    "- Also, an arrow followed by a data type is added to the end of the function signature. The general usage being: `<funcName(p1, ..., pn)> -> <returnVarType>`.  For example, `stringFunc(myStr : str) -> int` indicates a function that accepts one input parameter called `myStr` of type `str` that returns a value of `int` data type. This method more easily shows the return value types of any function or method, to avoid confusion by future developers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List, Any, Callable\n",
    "\n",
    "memo = dict()   # type: Dict[Tuple[str, str], int]\n",
    "def levenshtein(s: str, t: str) -> int:\n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    cost = 0 if s[-1] == t[-1] else 1\n",
    "\n",
    "    i1 = (s[:-1], t)\n",
    "    if i1 not in memo:\n",
    "        memo[i1] = levenshtein(*i1)\n",
    "    i2 = (s, t[:-1])\n",
    "    if i2 not in memo:\n",
    "        memo[i2] = levenshtein(*i2)\n",
    "    i3 = (s[:-1], t[:-1])\n",
    "    if i3 not in memo:\n",
    "        memo[i3] = levenshtein(*i3)\n",
    "    res = min([memo[i1] + 1, memo[i2] + 1, memo[i3] + cost])\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more general `get_neighbors()` that expects a distance function as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(training_set: List[Any],\n",
    "                  labels: List[Any],\n",
    "                  test_instance: Any,\n",
    "                  k: int,\n",
    "                  distance: Callable) -> List[Tuple[Any, float, Any]]:\n",
    "\n",
    "    distances = []\n",
    "    for index in range(len(training_set)):\n",
    "        dist = distance(test_instance, training_set[index])\n",
    "        distances.append((training_set[index], dist, labels[index]))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = distances[:k]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted by distance.  We will show this for the sci-kit learn code as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_distance_weights(neighbors: List[Tuple[Any, float, Any]], all_results: bool = True):\n",
    "    class_counter = Counter()\n",
    "    number_of_neighbors = len(neighbors)\n",
    "    for index in range(number_of_neighbors):\n",
    "        dist = neighbors[index][1]\n",
    "        label = neighbors[index][2]\n",
    "        class_counter[label] += 1 / (dist**2 + 1)\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    # print(labels, votes)\n",
    "    winner = class_counter.most_common(1)[0][0]\n",
    "    votes4winner = class_counter.most_common(1)[0][1]\n",
    "    if all_results:\n",
    "        total = sum(class_counter.values(), 0.0)\n",
    "        for key in class_counter:\n",
    "            class_counter[key] /= total\n",
    "        return winner, class_counter.most_common()\n",
    "    else:\n",
    "        return winner, votes4winner / sum(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vote probability, unweighted by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_prob(neighbors: List[Tuple[Any, float, Any]]):\n",
    "    class_counter = Counter()\n",
    "    for neighbor in neighbors:\n",
    "        class_counter[neighbor[2]] += 1\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    winner = class_counter.most_common()[0][0]\n",
    "    votes4winner = class_counter.most_common()[0][1]\n",
    "    return winner, votes4winner/sum(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check if there are any null values\n",
    "print(words_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the bad values\n",
    "words_df.dropna(inplace=True)\n",
    "print(words_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word.strip() for word in words_df[\"word\"])\n",
    "for word in [\"datq\", \"sceince\", \"holpful\", \"kundnoss\", \"holpposs\", \"blagrufoo\", \"qwerasdfzx\"]:\n",
    "    print(f\"{word:<10}:\")\n",
    "    k_neighbors = get_neighbors(words,\n",
    "                                words,\n",
    "                                word,\n",
    "                                3,\n",
    "                                distance=levenshtein)\n",
    "    print(f\"  neighbors: {[neighbor[1:] for neighbor in k_neighbors]}\")\n",
    "    print(f\"  vote_distance_weights: {vote_distance_weights(k_neighbors, all_results=False)}\")\n",
    "    print(f\"  vote_prob: {vote_prob(k_neighbors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Choose K\n",
    "\n",
    "By convention, many people try using `K = 5`.  But you should certainly perform a more scientific analysis.  The below code employs various `K` values and performs cross-validation using accuracy as the test metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_neighbors = list(range(1, 30, 2))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = [ ]\n",
    "\n",
    "#perform 10-fold cross-validation\n",
    "for k in num_neighbors:\n",
    "    knn4 = KNeighborsClassifier(n_neighbors = k,\n",
    "                               p=2\n",
    "                               metric = 'minkowski')\n",
    "\n",
    "    knn4.fit(normalized_training_data, iris_data.target)\n",
    "    \n",
    "    scores = cross_val_score(knn4,\n",
    "                             normalized_training_data,\n",
    "                             iris_data.target,\n",
    "                             cv = 10,\n",
    "                             scoring = 'accuracy'\n",
    "                            )\n",
    "    cv_scores.append(scores.mean())\n",
    "    \n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_df = pd.DataFrame({\"K\": [k for k in num_neighbors], \n",
    "              \"Accuracy\": cv_scores}).set_index(\"K\")\n",
    "print(cv_scores_df)\n",
    "cv_scores_df.plot.bar(figsize=(9,6), ylim=(0.9, 1.0), rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to misclassification error\n",
    "mse = [1 - x for x in cv_scores]\n",
    "\n",
    "# determing best k\n",
    "optimal_k = num_neighbors[mse.index(min(mse))]\n",
    "print(\"The optimal no. of neighbors is {}\".format(optimal_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance-Weighted KNN\n",
    "\n",
    "Perhaps a way to make the solution less sensitive to the 'right' choice of $k$ is to weight the value by distance.\n",
    "\n",
    "If we pick $k=5$, and the nearest neighbors give labels of \\[A, A, B, B, B\\], we would be inclided to pick the majority class of B.\n",
    "\n",
    "But what if the two A's were really close, and the three B's were somewhat further away?  We might pick A over B.\n",
    "\n",
    "We can use the `weights='distance'` hyperparameter, which weights data points by the inverse of their distance.  In this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list that will hold cv scores\n",
    "cv_scores = [ ]\n",
    "\n",
    "#perform 10-fold cross-validation\n",
    "for k in num_neighbors:\n",
    "    knn5 = KNeighborsClassifier(n_neighbors = k,\n",
    "                               p=2\n",
    "                               metric = 'minkowski'\n",
    "                               weights= 'distance')\n",
    "\n",
    "    knn5.fit(normalized_training_data, iris_data.target)\n",
    "    \n",
    "    scores = cross_val_score(knn5,\n",
    "                             normalized_training_data,\n",
    "                             iris_data.target,\n",
    "                             cv = 10,\n",
    "                             scoring = 'accuracy'\n",
    "                            )\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "cv_scores_df = pd.DataFrame({\"K\": [k for k in num_neighbors], \n",
    "                             \"Accuracy\": cv_scores}).set_index(\"K\")\n",
    "print(cv_scores_df)\n",
    "cv_scores_df.plot.bar(figsize=(9,6), ylim=(0.9, 1.0), rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'optimal' $k$ is essentially the same.  But the overall graph is much smoother, justifying the idea that weighting might lead to less sensitivity in choosing $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
