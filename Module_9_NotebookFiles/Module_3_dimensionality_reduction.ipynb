{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Dimensionality Reduction\n",
    "\n",
    "Many machine learning problems involve thousands or even millions of features for each training instance. Not only do all these features make training extremely slow, but they can also make it much harder to find a good solution, as we will see. This problem is often referred to as the _curse of dimensionality_.\n",
    "\n",
    "Fortunately, it is often possible to reduce the number of features considerably, which turns an intractable problem into a tractable one:\n",
    "\n",
    "MNIST Feature Importances obtained through Random Forest Algorithm:\n",
    "<img src=\"figures/MNISTFeatureImportances.png\" width =\"300\" />\n",
    "\n",
    "Image Compression performed via K-Means Algorithm:\n",
    "<img src=\"figures/imageCompression.png\" width =\"500\" />\n",
    "\n",
    "In this module we will discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space. Then, we will consider the two main approaches to dimensionality reduction (___projection___ and ___Manifold Learning___), and we will go through three of the most popular dimensionality reduction techniques: ___PCA___, ___KernelPCA___, and ___LLE___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"dim_reduction\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "High-dimensional spaces are hard to visualize!\n",
    "\n",
    "<img src=\"figures/dimensions.png \" width =\"600\" />\n",
    "\n",
    "Many things behave very differently in high-dimensional space.  Here are a couple examples:\n",
    "\n",
    "- For example, if you pick a random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located less than 0.001 from a border. But in a 10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the border.\n",
    "\n",
    "- If you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be roughly 0.66. If you pick two points randomly in a 1,000,000-dimensional unit hypercube? The average distance will be about 408.25. Wow.\n",
    "\n",
    "Let’s take a look at the two main approaches to reducing dimensionality: ___projection___ and ___Manifold Learning___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection Methods and Manifold Learning Overview\n",
    "\n",
    "### Projection\n",
    "\n",
    "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances lie within (or close to) a much lower-dimensional ___subspace___ of the high-dimensional space.\n",
    "\n",
    "Let's take a look at some data like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/dataset_3d_quick_view.png \" width =\"600\" />\n",
    "\n",
    "Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the high-dimensional (3D) space. If we project every training instance perpendicularly onto this subspace, then we get the new 2D dataset shown here:\n",
    "\n",
    "<img src=\"figures/dimensions_2D.png \" width =\"300\" />\n",
    "\n",
    "### Manifold Learning\n",
    "\n",
    "However, projection is not always the best approach to dimensionality reduction.  Consider the _Swiss roll_ toy dataset:\n",
    "\n",
    "<img src=\"figures/swiss_roll.png \" width =\"500\" />\n",
    "\n",
    "Simply projecting this data onto a plane would squash different layers of the Swiss roll together, as shown on the left side of the following figure. What you really want is to unroll the Swiss roll to obtain the 2D dataset on the right side of the following figure.\n",
    "\n",
    "<img src=\"figures/swiss_roll_proj.png \" width =\"800\" />\n",
    "\n",
    "The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold.  However, this implicit assumption does not always hold.\n",
    "\n",
    "In short, reducing the dimensionality of your training set before training a model will\n",
    "usually speed up training, but it may not always lead to a better or simpler solution; it\n",
    "all depends on the dataset.\n",
    "\n",
    "<img src=\"figures/swiss_roll_complex.png \" width =\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "___Principal Component Analysis (PCA)___ is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure pca_best_projection_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "np.random.seed(3)\n",
    "X = np.random.randn(m,2)/10\n",
    "X = X.dot(np.array([[stretch, 0], [0,1]])) # stretch\n",
    "X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]]) # rotate\n",
    "\n",
    "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
    "u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])\n",
    "u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])\n",
    "\n",
    "X_proj1 = X.dot(u1.reshape(-1, 1))\n",
    "X_proj2 = X.dot(u2.reshape(-1, 1))\n",
    "X_proj3 = X.dot(u3.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot2grid((3,2), (0, 0), rowspan=3)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], \"k-\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], \"k--\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], \"k:\", linewidth=2)\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", fontsize=22)\n",
    "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", fontsize=22)\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot2grid((3,2), (0, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=1)\n",
    "plt.plot(X_proj1[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot2grid((3,2), (1, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k--\", linewidth=1)\n",
    "plt.plot(X_proj2[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot2grid((3,2), (2, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\n",
    "plt.plot(X_proj3[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"pca_best_projection_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA using SVD Decomposition\n",
    "\n",
    "PCA identifies the direction that accounts for the largest amount of variance in the training set. It also finds a second direction, that is orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. The ith direction/axis is called the ___$i^{th}$ principal component (PC)___ of the data.\n",
    "\n",
    "Let's start by generating some data.  We are going to create the second figure shown in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can you find the principal components of a training set? There is a standard matrix factorization technique called _Singular Value Decomposition (SVD)_ (you are not responsible for understanding SVD) that can decompose the training set feature matrix $X$ into the matrix multiplication of three matrices $X = U \\Sigma V^T$, where $V$ contains the unit vectors that define all the principal components that we are looking for, as shown below:\n",
    "\n",
    "<img src=\"figures/principal_components.png \" width =\"200\" />\n",
    "\n",
    "Although the book does not discuss eigenvectors and eigenvalues, PCA is actually an eigen-problem.  The principal components of the feature matrix $X$ are actually eigenvectors of the covariance matrix of $X$, and the eigenvalues indicate the amount of variance explained by its associated eigenvector.  You are not responsible for understanding eigenvalue problems.\n",
    "\n",
    "Let's find the first two principal components of our data, and prove to ourselves that SVD works properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "\n",
    "S = np.zeros(X_centered.shape)\n",
    "S[:n, :n] = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The allclose() function returns `True` if two arrays are element-wise equal within a tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_centered, u.dot(S).dot(vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To project the training set onto the lower-dimensional hyperplane and obtain a reduced dataset of dimensionality $d$, compute the matrix multiplication of the training set matrix $X$ by the matrix $W_d$, defined as the matrix containing the first $d$ columns of $V$, as shown in the following equation:\n",
    "\n",
    "<img src=\"figures/projection.png \" width =\"200\" />\n",
    "\n",
    "The following Python code projects the training set onto the plane defined by the first\n",
    "two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D_using_svd = X2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to fit a PCA transformer.  With Scikit-Learn, doing PCA is really trivial. It even takes care of mean centering for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decompositionimport PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D_using_SVD[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that running PCA multiple times on slightly different datasets may result in different results. In general the only difference is that some axes may be flipped. In this example, PCA using Scikit-Learn gives the same projection as the one given by the SVD approach, except both axes are flipped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X2D, -X2D_using_SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the principal components? The `PCA` object gives access to the principal components that it computed through the `components_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the first two principal components computed using the SVD method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the axes are flipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n",
    "\n",
    "The ratio indicates the proportion of the dataset’s variance that lies along each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the first PC explains 84.2% of the variance, while the second explains 14.6%.  By projecting down to 2 dimensions, we lost about 1.1% of the variance in the data, shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to compute the explained variance ratio using the SVD approach (recall that `s` is the diagonal of the matrix `S`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(s) / np.square(s).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovered Data\n",
    "\n",
    "Can we recover the 3D data? Yes! To recover the 3D points projected on the plane (PCA 2D subspace), use the `inverse_transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3D_inv = pca.inverse_transform(X2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there was some loss of information during the projection step, so the recovered 3D points are not exactly equal to the original 3D points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X3D_inv, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the reconstruction error (which is the MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sum(np.square(X3D_inv - X), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse transform in the SVD approach looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3D_inv_using_svd = X2D_using_svd.dot(Vt[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstructions from both methods are not identical because Scikit-Learn's `PCA` class automatically takes care of reversing the mean centering, but if we subtract the mean, we get the same reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X3D_inv_using_svd, X3D_inv - pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate some nice figures.  Note that this utility class to draw 3D arrows was copied from [Stack Overflow](http://stackoverflow.com/questions/11140163)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Express the plane as a function of x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [-1.8, 1.8, -1.3, 1.3, -1.0, 1.0]\n",
    "\n",
    "x1s = np.linspace(axes[0], axes[1], 10)\n",
    "x2s = np.linspace(axes[2], axes[3], 10)\n",
    "x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "C = pca.components_\n",
    "R = C.T.dot(C)\n",
    "z = (R[0, 2] * x1 + R[1, 2] * x2) / (1 - R[2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 3D dataset, the plane and the projections on that plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3.8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "X3D_above = X[X[:, 2] > X3D_inv[:, 2]]\n",
    "X3D_below = X[X[:, 2] <= X3D_inv[:, 2]]\n",
    "\n",
    "ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"bo\", alpha=0.5)\n",
    "\n",
    "ax.plot_surface(x1, x2, z, alpha=0.2, color=\"k\")\n",
    "np.linalg.norm(C, axis=0)\n",
    "ax.add_artist(Arrow3D([0, C[0, 0]],[0, C[0, 1]],[0, C[0, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|>\", color=\"k\"))\n",
    "ax.add_artist(Arrow3D([0, C[1, 0]],[0, C[1, 1]],[0, C[1, 2]], mutation_scale=15, lw=1, arrowstyle=\"-|>\", color=\"k\"))\n",
    "ax.plot([0], [0], [0], \"k.\")\n",
    "\n",
    "for i in range(m):\n",
    "    if X[i, 2] > X3D_inv[i, 2]:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], \"k-\")\n",
    "    else:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]], [X[i][1], X3D_inv[i][1]], [X[i][2], X3D_inv[i][2]], \"k-\", color=\"#505050\")\n",
    "    \n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"k+\")\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"k.\")\n",
    "ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"bo\")\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18, labelpad=10)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18, labelpad=10)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18, labelpad=10)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "# Note: If you are using Matplotlib 3.0.0, it has a bug and does not\n",
    "# display 3D graphs properly.\n",
    "# See https://github.com/matplotlib/matplotlib/issues/12239\n",
    "# You should upgrade to a later version. If you cannot, then you can\n",
    "# use the following workaround before displaying each 3D graph:\n",
    "# for spine in ax.spines.values():\n",
    "#     spine.set_visible(False)\n",
    "\n",
    "save_fig(\"dataset_3d_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k+\")\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k.\")\n",
    "ax.plot([0], [0], \"ko\")\n",
    "ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "ax.set_xlabel(\"$z_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "ax.axis([-1.5, 1.3, -1.2, 1.2])\n",
    "ax.grid(True)\n",
    "save_fig(\"dataset_2d_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning\n",
    "\n",
    "(This section merely recreates the figures we discussed above.) Let's revisit the figures we saw previously.  We will create those figures here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)\n",
    "ax.view_init(10, -70)\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "save_fig(\"swiss_roll_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.axis(axes[:4])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(t, X[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"squished_swiss_roll_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "x2s = np.linspace(axes[2], axes[3], 10)\n",
    "x3s = np.linspace(axes[4], axes[5], 10)\n",
    "x2, x3 = np.meshgrid(x2s, x3s)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "\n",
    "positive_class = X[:, 0] > 5\n",
    "X_pos = X[positive_class]\n",
    "X_neg = X[~positive_class]\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot_wireframe(5, x2, x3, alpha=0.5)\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "save_fig(\"manifold_decision_boundary_plot1\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.plot(t[positive_class], X[positive_class, 1], \"gs\")\n",
    "plt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"manifold_decision_boundary_plot2\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(111, projection='3d')\n",
    "\n",
    "positive_class = 2 * (t[:] - 4) > X[:, 1]\n",
    "X_pos = X[positive_class]\n",
    "X_neg = X[~positive_class]\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "save_fig(\"manifold_decision_boundary_plot3\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "plt.plot(t[positive_class], X[positive_class, 1], \"gs\")\n",
    "plt.plot(t[~positive_class], X[~positive_class, 1], \"y^\")\n",
    "plt.plot([4, 15], [0, 22], \"b-\", linewidth=2)\n",
    "plt.axis([4, 15, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"manifold_decision_boundary_plot4\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Right Number of Dimensions and MNIST Compression\n",
    "\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance (i.e. 95%).\n",
    "\n",
    "Let's start by importing the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set’s variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could then set n_components=d and run PCA again. But there is a much better option: instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_componenents_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another option is to plot the explained variance as a function of the number of dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
    "plt.grid(True)\n",
    "save_fig(\"explained_variance_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to decompress the reduced dataset by applying the inverse transformation of the PCA projection. This won’t give you back the exact original data, but it will be close. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the ___reconstruction error___.\n",
    "\n",
    "The following code compresses the MNIST dataset down to 154 dimensions, then uses the inverse_transform() method to decompress it back to 784 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    # This is equivalent to n_rows = ceil(len(instances) / images_per_row):\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # Append empty images to fill the end of the grid, if needed:\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n",
    "\n",
    "    # Reshape the array so it's organized as a grid containing 28×28 images:\n",
    "    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n",
    "\n",
    "    # Combine axes 0 and 2 (vertical image grid axis, and vertical image axis),\n",
    "    # and axes 1 and 3 (horizontal axes). We first need to move the axes that we\n",
    "    # want to combine next to each other, using transpose(), and only then we\n",
    "    # can reshape:\n",
    "    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size,\n",
    "                                                         images_per_row * size)\n",
    "    # Now that we have a big image, we just need to show it:\n",
    "    plt.imshow(big_image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "\n",
    "save_fig(\"mnist_compression_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_pca = X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of the inverse transformation is:\n",
    "\n",
    "$$ \\mathbf{X}_{recovered} = \\mathbf{X}_{d\\_proj}\\mathbf{W}_{d}^{T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized PCA\n",
    "\n",
    "If you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a stochastic algorithm called ___Randomized PCA___ that quickly finds an approximation of the first d principal components.\n",
    "\n",
    "The computational complexity of randomized PCA is $O(m \\cdot d^2) + O(d^3)$, whereas full SVD PCA is $O(m \\cdot n^2) + O(n^3)$.  So, randomized PCA is dramatically faster than full SVD when d is much smaller than n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA \n",
    "\n",
    "One problem with the PCA implementations we have discussed so far is that they require the whole training set to fit in memory in order for the algorithm to run. Fortunately, ___Incremental PCA (IPCA)___ algorithms allow you to split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decompositionimport IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    print(\".\", end=\"\")\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered_inc_pca[::2100])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_inc_pca = X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results of transforming MNIST using regular PCA and incremental PCA. First, the means are equal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(pca.mean_, inc_pca.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the results are not exactly identical. Incremental PCA gives a very good approximate solution, but it's not perfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_reduced_pca, X_reduced_inc_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time regular PCA against Incremental PCA and Randomized PCA, for various number of principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for n_components in (2, 10, 154):\n",
    "    print(\"n_components =\", n_components)\n",
    "    regular_pca =PCA(n_components = n_components)\n",
    "    inc_pca = IncrementalPCA(n_components = n_components, batch_siz(e = 500)\n",
    "    rnd_pca PCA(n_components = n_components, svd_solver=\"radnomized\", random_state=42)\n",
    "\n",
    "    for pca in (regular_pca, inc_pca, rnd_pca):\n",
    "        t1 = time.time()\n",
    "        pca.fit(X_train)\n",
    "        t2 = time.time()\n",
    "        print(\"    {}: {:.1f} seconds\".format(pca.__class__.__name__, t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "\n",
    "Last semester when studying SVMs, we discussed the kernel trick, which is a mathematical technique that implicitly maps instances into a very high-dimensional space (called the feature space) and enables nonlinear classification and regression. Recall that a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space.\n",
    "\n",
    "It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called ___Kernel PCA (kPCA)___. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X,t = make_swiss_roll(n_samples = 1000, noise = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "lin_pca =  KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform =True, gamma=0.04)\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform =True)\n",
    "sig_pca =  KernelPCA(n_components = 2, kernel=\"sigmoid\", fit_inverse_transform =True, coefo=0)\n",
    "\n",
    "\n",
    "y = t > 6.9\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    if subplot == 132:\n",
    "        X_reduced_rbf = X_reduced\n",
    "    \n",
    "    plt.subplot(subplot)\n",
    "    #plt.plot(X_reduced[y, 0], X_reduced[y, 1], \"gs\")\n",
    "    #plt.plot(X_reduced[~y, 0], X_reduced[~y, 1], \"y^\")\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "save_fig(\"kernel_pca_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "(\"kpca\", KernelPCA(n_components=2)),\n",
    "(\"log_reg\", LogisticRegression())\n",
    "])\n",
    "param_grid = [{\n",
    "\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "\"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLE\n",
    "\n",
    "___Locally Linear Embedding (LLE)___ is another powerful ___nonlinear dimensionality reduction (NLDR)___ technique. It is a manifold learning technique that does not rely on projections, like the previous algorithms do. In a nutshell, LLE works by first measuring how each training instance linearly relates to its closest neighbors, and then looking for a low-dimensional representation of the training set where these local relationships are best preserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses Scikit-Learn’s LocallyLinearEmbedding class to unroll the Swiss roll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Unrolled swiss roll using LLE\", fontsize=14)\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18)\n",
    "plt.axis([-0.065, 0.055, -0.1, 0.12])\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"lle_unrolling_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS, Isomap and t-SNE\n",
    "\n",
    "Other dimensionality reduction techniques include:\n",
    "\n",
    "- Multidimensional Scaling (MDS)\n",
    "- Isomap\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Random Projections\n",
    "\n",
    "A few of examples of these methods are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2, random_state=42)\n",
    "X_reduced_mds = mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "isomap = Isomap(n_components=2)\n",
    "X_reduced_isomap = isomap.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_mnist = mnist[\"data\"]\n",
    "y_mnist = mnist[\"target\"]\n",
    "lda.fit(X_mnist, y_mnist)\n",
    "X_reduced_lda = lda.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"MDS\", \"Isomap\", \"t-SNE\"]\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "for subplot, title, X_reduced in zip((131, 132, 133), titles,\n",
    "                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):\n",
    "    plt.subplot(subplot)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "save_fig(\"other_dim_reduction_plot\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
