{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Classification\n",
    "\n",
    "In Module 1, we learned that the most common supervised learning tasks are ___regression___ (predicting values) and ___classification___ (predicting classes). In module 2 we explored a regression task, predicting housing values, using various algorithms such as Linear Regression, Decision Trees, and Random Forests. Now we will turn our attention to ___classification systems___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "\n",
    "\"MNIST\" in the MNIST dataset stands for Modified National Institute of Standards and Technology.  The MNIST dataset contains images of handwritten digits collected from Census Bureau employees and highschool students. The first 60,000 examples of the data set form the training data set, and the last 10,000 examples form the testing data set.  More information can be found [here](http://yann.lecun.com/exdb/mnist/) and [here](https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets loaded by Scikit-Learn generally have a similar dictionary structure.  We can view the keys of the dictionary using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the entities we are working with.  More information from about the n-dimensional array called ndarray can be found [here](https://numpy.org/doc/stable/reference/arrays.ndarray.html#:~:text=An%20ndarray%20is%20a%20(usually,the%20sizes%20of%20each%20dimension.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each image is a 28 pixels x 28 pixels. \n",
    "28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[0]\n",
    "print(some_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[0]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap=mpl.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "save_fig(\"some_digit_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting multiple images\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "save_fig(\"more_digits_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary classifier\n",
    "\n",
    "Let's train a model that is a “5-detector”, which is a binary classifier that is capable of\n",
    "distinguishing between just two classes, 5 and not-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True for all 5s, False for all other digits \n",
    "### ENTER CODE HERE ### y_train\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click for more information about [Stochastic Gradient Descent Classifiers and Regressors](https://scikit-learn.org/stable/modules/sgd.html#sgd).\n",
    "\n",
    "SGD is an optimization technique, and does not correspond to a specific family of machine learning models. It is only a way to train a model. For example, `SGDClassifier(loss='log')` is equivalent to `LogisticRegression` that is fitted via SGD. `SGDRegressor(loss='squared_loss', penalty='l2')` is equivalent to `Ridge` that is fitted via SGD.\n",
    "\n",
    "We are using many of the default values.  We use the default loss function, which is 'hinge' and implies that a linear SVM will be used.  The default penalty, or regularization term used is 'L2', which is the standard regularizer for linear SVM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measures\n",
    "\n",
    "Evaluating a classifier is often significantly trickier than evaluating a regressor.\n",
    "\n",
    "## Measuring Accuracy Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you get too excited, let’s look at a very dumb classifier that just classifies every single image in the “not-5” class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s right, it has over 90% accuracy! This is simply because only about 10% of the\n",
    "images are 5s, so if you always guess that an image is not a 5, you will be right about\n",
    "90% of the time. \n",
    "\n",
    "This demonstrates why accuracy is generally not the preferred performance measure\n",
    "for classifiers, especially when you are dealing with ___skewed (or imbalanced) datasets___ (i.e., when some\n",
    "classes are much more frequent than others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the [confusion\n",
    "matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).  Let's start by getting predictions from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, how does cross_val_predict() make a prediction?\n",
    "\n",
    "![KFoldCrossValidation.png](Figures/KFoldCrossValidation.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5  # pretend we reached perfection\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix again using the plot_confusion_matrix() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# formatting\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot Options for Confusion Matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(sgd_clf, X_train, y_train_5,\n",
    "                                 display_labels=[\"Not a 5\", \"5\"],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a slightly different look at this Confusion Matrix:\n",
    "\n",
    "![confusionMatrixExample](Figures/confusionMatrixExample.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and Error Rate\n",
    "\n",
    "When testing a classifier on an example whose real class is known, we can encounter only the following four different outcomes:\n",
    "\n",
    " - ___True Positive___: the example is positive and the classifier correctly recognizes it as such\n",
    " - ___True Negative___: the example is negative and the classifier correctly recognizes it as such\n",
    " - ___False Negative___: the example is positive, but the classifier labels it as negative\n",
    " - ___False Positive___: the example is negative, but the classifier labels it as positive\n",
    "\n",
    "A classifier's ___error rate (E)___ is the frequency of errors made by the classifier over a given set of examples:\n",
    "\n",
    "$$ E = \\frac{N_{FP} + N_{FN}}{N_{FP}+N_{FN}+N_{TP}+N_{TN}} $$\n",
    "\n",
    "A classifier's ___accuracy (Acc)___ is the frequency of correct classifications made by the classifier over a given set of examples:\n",
    "\n",
    "$$ Acc = 1 - E = \\frac{N_{TP} + N_{TN}}{N_{FP}+N_{FN}+N_{TP}+N_{TN}} $$\n",
    "\n",
    "In the case of binary classification problems, by convention, data scientists assign the class with fewer instances to be the \"positive\" class, and identify it with unity, or 1. Further, the class with more instances is considered to be the \"negative\" class, and it is identified by 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "In some applications, negative examples far outnumber positive examples by a wide margin - this is what we call a ___class imbalance___.  When this happens, error rate and accuracy offer a misleading picture of classification performance.\n",
    "\n",
    "$$ precision= Pr = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "where TP is the number of true positives, and FP is the number of false positives\n",
    "\n",
    "$$ recall = Re = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "where TP is the number of true positives, FN is the number of false negatives.\n",
    "\n",
    "$$ F_{\\beta} = \\frac{(\\beta^2+1) \\cdot Pr \\cdot Re}{\\beta^2 \\cdot Pr + Re} $$\n",
    "\n",
    "where $\\beta$ is the number of times that recall is considered to be more important than precision.  If precision and recall are equally as important, than we have $F_1$ that is the ___harmonic mean___ of precision and recall. \n",
    "\n",
    "$$ F_{1} = 2 \\cdot \\frac{Pr \\cdot Re}{Pr + Re} $$\n",
    "\n",
    "Let's calculate these metrics below.  First, lets remind ourselves of the confusion matrix values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [precision_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3530 / (3530 + 687)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [recall_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3530 / (3530 + 1891)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [f1_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3530 / (3530 + (687 + 1891) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall Trade-Off\n",
    "\n",
    "If someone says to you, “Let’s reach 99% precision,” you should ask, “At what recall?”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![precisionRecallTradeOff](Figures/precisionRecallTradeOff.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [decision_function method](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function) returns scores returned through evaluation by the decision fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 8000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the [cross_val_predict method]() to get scores for each instances in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [precision_recall_curve() method](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) computes precision-recall pairs for different probability thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([-50000, 50000, 0, 1])             # Not shown\n",
    "\n",
    "\n",
    "\n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))                                                                  # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\n",
    "save_fig(\"precision_recall_vs_threshold_plot\")                                              # Not shown\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train_pred == (y_scores > 0)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.plot([0.4368, 0.4368], [0., 0.9], \"r:\")\n",
    "plt.plot([0.0, 0.4368], [0.9, 0.9], \"r:\")\n",
    "plt.plot([0.4368], [0.9], \"ro\")\n",
    "save_fig(\"precision_vs_recall_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores >= threshold_90_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curves\n",
    "\n",
    "The ___receiver operating characteristic (ROC) curve___ plots the ___true positive rate___ (another name for recall) against the ___false positive rate (FPR)___. The FPR is the ratio of negative instances that are incorrectly classified as positive.  \n",
    "\n",
    "Scikit-Learn provides the [roc_curve() method](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) for binary classification tasks to compute the ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 6))                         # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\") # Not shown\n",
    "plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")  # Not shown\n",
    "plt.plot([4.837e-3], [0.4368], \"ro\")               # Not shown\n",
    "save_fig(\"roc_curve_plot\")                         # Not shown\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now train a RandomForestClassifier and compare its ROC curve and ROC AUC score to those of the SGDClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "### ENTER CODE HERE ###\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\")\n",
    "plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")\n",
    "plt.plot([4.837e-3], [0.4368], \"ro\")\n",
    "plt.plot([4.837e-3, 4.837e-3], [0., 0.9487], \"r:\")\n",
    "plt.plot([4.837e-3], [0.9487], \"ro\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "save_fig(\"roc_curve_comparison_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
    "precision_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train_5, y_train_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification\n",
    "\n",
    "___Binary classifiers___ distinguish only between two classes, however ___multiclass classifiers___ (also called ___multinomial classifiers___) can distinguish between more than two classes.\n",
    "\n",
    "Two strategies that allow for multiclass classification using multiple binary classifiers are called:\n",
    "\n",
    " - ___one-versus-the-rest (OvR)___\n",
    " - ___one-versus-one (OvO)___\n",
    "\n",
    "Let's use a [Support Vector Classifier (SVC)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) to try building a classifier that can distinguish between all 10 classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "# y_train, not y_train_5\n",
    "### ENTER CODE HERE ###\n",
    "svm_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = svm_clf.decision_function([some_digit])\n",
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(some_digit_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.classes_[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as one-vs-all, the [One-vs-the-rest (OvR) multiclass/multilabel strategy](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) consists in fitting one classifier per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "### ENTER CODE HERE ###\n",
    "ovr_clf.fit(X_train[:1000], y_train[:1000])\n",
    "ovr_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ovr_clf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "We now will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "### ENTER CODE HERE ###\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since sklearn 0.22, you can use sklearn.metrics.plot_confusion_matrix()\n",
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "save_fig(\"confusion_matrix_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "save_fig(\"confusion_matrix_errors_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_a, cl_b = 3, 5\n",
    "\n",
    "X_temp = X_train[:3000].copy()\n",
    "\n",
    "X_aa = X_temp[(y_train[:3000] == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_temp[(y_train[:3000] == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_temp[(y_train[:3000] == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_temp[(y_train[:3000] == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
    "save_fig(\"error_analysis_digits_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# formatting\n",
    "np.set_printoptions(precision=1)\n",
    "\n",
    "# Plot Options for Confusion Matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(sgd_clf, X_train[:3000], y_train[:3000],\n",
    "                                 display_labels=[0, 1, 3, 4, 5, 6, 7, 8, 9],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    \n",
    "    disp.ax_.set_title(title)\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification\n",
    "\n",
    "A classification system that outputs multiple binary tags is called a ___multilabel classification system___.\n",
    "\n",
    "\n",
    "Let's take a look at a [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html):\n",
    "\n",
    "\n",
    "Here is a refresher on [modular arithmetic](https://www.khanacademy.org/computing/computer-science/cryptography/modarithmetic/a/what-is-modular-arithmetic) if you need one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = ### ENTER CODE HERE ###\n",
    "y_train_odd = ### ENTER CODE HERE ###\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the following cell may take a very long time (possibly hours depending on your hardware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multioutput Classification\n",
    "\n",
    "The last type of classification task we are going to discuss here is called ___multioutput–multiclass classification___ (or simply ___multioutput classification___). It is simply a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).\n",
    "\n",
    "We will use numpy's [randint()](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html) function to generate some random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "### ENTER CODE HERE ###\n",
    "### ENTER CODE HERE ###\n",
    "### ENTER CODE HERE ###\n",
    "### ENTER CODE HERE ###\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_index = 0\n",
    "plt.subplot(121); plot_digit(X_test_mod[some_index])\n",
    "plt.subplot(122); plot_digit(y_test_mod[some_index])\n",
    "save_fig(\"noisy_digit_example_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
    "plot_digit(clean_digit)\n",
    "save_fig(\"cleaned_digit_example_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
